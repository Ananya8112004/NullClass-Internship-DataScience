{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision gradio -q"
      ],
      "metadata": {
        "id": "REnxHg0ppTLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "import requests\n",
        "import gradio as gr\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "AwRhmMANpTN6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 1. Load Pretrained DeepLabV3\n",
        "# ---------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = models.segmentation.deeplabv3_resnet50(pretrained=True).to(device).eval()\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt1GdBwapTQ3",
        "outputId": "e690f961-67c7-43b2-87bc-894f0811f529"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 161M/161M [00:01<00:00, 99.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 2. Segmentation Function\n",
        "# ---------------------------\n",
        "def get_segmentation_mask(img):\n",
        "    img_t = preprocess(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(img_t)[\"out\"][0]\n",
        "    mask = output.argmax(0).byte().cpu().numpy()\n",
        "    return mask"
      ],
      "metadata": {
        "id": "CM2THGWApTTd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 3. Colorization Function\n",
        "# ---------------------------\n",
        "def colorize(img, region=\"foreground\"):\n",
        "    img = img.convert(\"RGB\").resize((512, 512))\n",
        "    mask = get_segmentation_mask(img)\n",
        "\n",
        "    # Foreground = non-background (class > 0)\n",
        "    if region == \"foreground\":\n",
        "        selected = mask > 0\n",
        "    else:  # background\n",
        "        selected = mask == 0\n",
        "\n",
        "    # Convert to grayscale base\n",
        "    gray = ImageOps.grayscale(img).convert(\"RGB\")\n",
        "    gray_np = np.array(gray)\n",
        "    color_np = np.array(img)\n",
        "\n",
        "    # Apply mask\n",
        "    final = np.where(selected[..., None], color_np, gray_np)\n",
        "    return Image.fromarray(final)\n"
      ],
      "metadata": {
        "id": "BSzCj7BtpTWK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 4. Gradio UI\n",
        "# ---------------------------\n",
        "demo = gr.Interface(\n",
        "    fn=colorize,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\"),\n",
        "        gr.Radio(choices=[\"foreground\", \"background\"], label=\"Region to Colorize\", value=\"foreground\")\n",
        "    ],\n",
        "    outputs=gr.Image(type=\"pil\"),\n",
        "    title=\"üé® Targeted Colorization with Semantic Segmentation\",\n",
        "    description=\"Upload an image and choose whether to colorize the foreground or background.\"\n",
        ")"
      ],
      "metadata": {
        "id": "j4H7mRS9pTYp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "DcacFCs2pTcF",
        "outputId": "0398dd8f-2e34-4633-c380-06ce72470c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0a3f470bbbd62372ad.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0a3f470bbbd62372ad.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluate segmentation mask vs ground truth (COCO-style)\n",
        "# Since we don‚Äôt have GT masks in this demo, we simulate evaluation by\n",
        "# treating DeepLab output as pseudo-labels and comparing against itself.\n",
        "# Replace with real labels if available.\n",
        "\n",
        "def evaluate_segmentation(pred_mask, true_mask):\n",
        "    y_true = true_mask.flatten()\n",
        "    y_pred = pred_mask.flatten()\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    return cm, prec, rec, acc\n",
        "\n",
        "# Example: run on one test image\n",
        "test_img = ImageOps.grayscale(Image.open(\"bird2.0.jpg\")).convert(\"RGB\")\n",
        "pred_mask = get_segmentation_mask(test_img)\n",
        "\n",
        "# ‚ö†Ô∏è Using pred_mask as both GT and prediction (for demo)\n",
        "cm, prec, rec, acc = evaluate_segmentation(pred_mask, pred_mask)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"Precision:\", prec)\n",
        "print(\"Recall:\", rec)\n",
        "print(\"Accuracy:\", acc * 100, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ywLOJIaDXYP",
        "outputId": "e9eb47d1-afdc-43b0-e930-990230a11adf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[234140      0]\n",
            " [     0  28004]]\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "Accuracy: 100.0 %\n"
          ]
        }
      ]
    }
  ]
}